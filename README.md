# Store Data Generator for Azure Event Hubs and Azure HDInsight Kafka
---
## Description  
  
This is a grocery store data generator for emulating real-time POS transactions and sending them to either Azure Event Hubs or Apache Kafka (test with Azure HDInsight Kafka). Using modified datasets based on the ones provided by the [Open Grocery Database Project](http://www.grocery.com/open-grocery-database-project/), the scripts do the following:  
### Scripts  
- `batchSalesGenerator.py`: This is a configurable script that can be used to generate monthly transaction data for a variable number of stores. By changing the date ranges and months in the script and configuring the storedata.csv to add or subtract the number of stores the script can generate different amounts of data for different time ranges. Writes data to local computer running the script.  
    - Amount of Data Created Per Store: approx. 181 MB (i.e. 30 stores == 5.4 GB of data generated)    
    - Sample View of the Data:  
```
transactionID,transactionDate,productID,category,price,brand,productName,zipcode,state
1000-32021-Y,1561939200,281,PERSONALCARE,13,Ogx,Ogx Body Wash Hydrating Argan Oil Of Morocco,90001,CA
1000-32021-Y,1561939200,140,OFFICESUPPLIES,10,Swingline,Swingline Standard Desk Stapler Bonus Pack!,90001,CA
```  
- `salesGenerator.py`: This is a script that can be configured to either stream generated data to Azure Event Hubs or Apach Kafka (tested with Azure HDInsight Kafka); additionally, the data generated follows the same format as the data generated by the `batchSalesGenerator.py` so they can be used together to set up a demo Lambda Architecture.  
    - Sample Schema of generated JSON:  
```
{
    'transactionID': string 
    'transactiondate': unix timestamp
    'productid': int
    'category': string
    'price': int
    'brand': string
    'storeId': int
    'productname': string
}
```  
### CSVs  
- `GROCERY_DATA.csv`: A list of 300 products that are commonly sold in grocery stores. The prices were randomly generated (so they may not always make sense). The point is not accuracy to real-world prices, but rather accuracy in terms of the type and variety of products. The CSV can always be adjusted to create more realistic prices if so desired.  
- `storedata.csv`: An initial list of 30 stores with zip code and location information that can help with demo visualizations of the data and demo analysis of the data if one chooses to do geographic-based analysis/visualizations.

## Set Up
  
### Batch  
  
No set up required. Simply open your terminal or Command Prompt and go to the directory that houses this repository and run the following: `python batchSalesGenerator.py`  
  
### Stream
  
#### Event Hubs Configuration  
  
**NOTE**: You must be using the azure-servicebus 0.21.1 version not 0.50.0 on your local machine. If you have never installed this package, then simply run: `pip install azure-servicebus==0.21.1` to install the correct package. **This code will not run if it is using the wrong package version**.  
  
1. Take the Event Hub Namespace and enter it into `EVENT_HUB_NAMESPACE`  
2. Go to "Shared Access Policies" in the Event Hub namespace page in the Azure Portal  
3. Create a new key by clicking the "+ Add" button near the top of the blade  
4. Give it a name and give it "Send" permissions and then click "Create"  
5. Once created, use the name of the Key in `SHARED_ACCESS_KEY_NAME`  
6. Then click on the key in the Azure Portal to retireve the Primary Key and copy and paste that into `KEY_VALUE`  
7. Take the name of the Event Hub under the Event Hub Namespace that was created that you want to use and enter it into `EVENT_HUB_NAME`
  
#### Kafka Configuration (Azure HDInsight Kafka-based)  
  
1. Get IP addresses from two of the worker nodes and enter them into `WORKER_IP_A` and `WORKER_IP_B`  
2. Then enter the name of the Kafka topic that you want to stream the data to into `TOPIC_NAME`
  
## TODO
  
Open to contributors, some areas I want to improve:  
- Make the batch data generator more easily configurable (i.e. auto generate the date ranges)
- Add more unique scenarios:  
    - Like abberrant data generation  
    - Fraud Detection  
    - etc...
